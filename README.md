# SoC
# Topic: RL: Race and Learn (ID:90)
## Week 1:
Focused on strengthening Python fundamentals, including variables, loops, functions, and basic data structures. Also explored the Pygame library to understand how to handle graphics, game loops, and user input. As a hands-on project, developed a playable Snake game, which involved implementing game logic, collision detection, and score tracking and button implementation. This helped reinforce both Python syntax and event-driven programming concepts.
The complete snake game code is included in this repository.

## Weeks 2:
Explored the PyTorch library and studied the fundamentals of neural networks and convolutional neural networks (CNNs). As an assignment, implemented an MNIST digit classifier using PyTorch, covering data loading, model architecture, training, and evaluation. As part of the assignment, also prepared a brief report documenting the architecture, training results, and key observations from the implementation.
The full code and report are included in this repository.

## Week 3
Studied the fundamentals of **Reinforcement Learning (RL)** through the renowned YouTube lecture series by **David Silver** (Google DeepMind).
Covered a wide range of core RL concepts, including:
- **Markov Decision Processes (MDPs)** and **Markov Reward Processes (MRPs)**
- **Value functions** and **Bellman equations**
- **Synchronous** and **Asynchronous Dynamic Programming**
- **Policy evaluation** and **policy control methods**
- **Principle of Optimality**
- **Monte Carlo learning**
- **Temporal Difference Learning (TD(0))**, including **forward view** and **backward view TD(λ)**
- **On-policy** and **Off-policy learning**
- **Q-learning** and **SARSA(λ)**
- **Value function approximation methods**
- **Policy gradient methods** and the **Actor-Critic method**
- **Dyna** and **Dyna-2 algorithms**
- **Multi-Armed Bandits**, including **ε-greedy** and **decaying ε-greedy** policies
- **Upper and Lower Confidence Bounds (UCB)** and the **UCB1 algorithm**

This week provided a strong theoretical foundation in RL and improved my understanding of how intelligent agents learn through interaction with their environment.
